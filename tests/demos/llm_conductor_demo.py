"""
LLM-Integrated Conductor Service

This module provides a complete conductor service that integrates with OpenRouter LLM
to automatically optimize and manage graph-based applications. It uses the UDP reliability
system and demonstrates the full LLMFlow architecture.
"""

import os
import json
import asyncio
import logging
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path

from llmflow.conductor.manager import ConductorManager
from llmflow.queue.manager import QueueManager
from llmflow.atoms.openrouter_llm import (
    OpenRouterServiceAtom, 
    OpenRouterRequest, 
    OpenRouterRequestAtom
)
from llmflow.atoms.data import StringAtom
from llmflow.core.base import ComponentType

logger = logging.getLogger(__name__)


class LLMIntegratedConductor:
    """Conductor service with full LLM integration for automated optimization."""
    
    def __init__(self, openrouter_key: Optional[str] = None):
        """Initialize the LLM-integrated conductor."""
        
        # Set up OpenRouter API key
        self.api_key = (
            openrouter_key or 
            os.getenv('OPENROUTER_API_KEY') or 
            "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"
        )
        os.environ['OPENROUTER_API_KEY'] = self.api_key
        
        # Initialize core services
        self.queue_manager = QueueManager()
        self.conductor_manager = ConductorManager(self.queue_manager)
        self.llm_service = OpenRouterServiceAtom()
        
        # Current deployed applications
        self.deployed_apps: Dict[str, Dict[str, Any]] = {}
        
        # LLM optimization configuration
        self.llm_config = {
            'model': 'google/gemini-2.0-flash-001',  # Only Gemini Flash as requested
            'optimization_interval': 60.0,  # Check for optimizations every minute
            'auto_apply_threshold': 0.85,   # Auto-apply optimizations with >85% confidence
            'max_optimization_attempts': 3,
            'performance_degradation_threshold': 0.2  # 20% performance drop triggers optimization
        }
        
        # Service state
        self.running = False
        self.optimization_task: Optional[asyncio.Task] = None
        
        # Statistics
        self.stats = {
            'apps_deployed': 0,
            'optimizations_performed': 0,
            'auto_optimizations_applied': 0,
            'performance_improvements': 0,
            'total_llm_cost': 0.0,
            'uptime_start': None
        }
        
        logger.info("LLM-Integrated Conductor initialized with Gemini 2.0 Flash")
    
    async def start(self) -> None:
        """Start the LLM-integrated conductor service."""
        if self.running:
            return
        
        logger.info("ğŸš€ Starting LLM-Integrated Conductor Service")
        
        # Start core services
        await self.queue_manager.start()
        await self.conductor_manager.start()
        
        # Start LLM optimization loop
        self.optimization_task = asyncio.create_task(self._llm_optimization_loop())
        
        # Register for conductor events
        self.conductor_manager.add_event_handler('process_started', self._on_process_started)
        self.conductor_manager.add_event_handler('process_stopped', self._on_process_stopped)
        
        self.running = True
        self.stats['uptime_start'] = datetime.utcnow()
        
        logger.info("âœ… LLM-Integrated Conductor Service started")
    
    async def stop(self) -> None:
        """Stop the LLM-integrated conductor service."""
        if not self.running:
            return
        
        logger.info("ğŸ›‘ Stopping LLM-Integrated Conductor Service")
        
        self.running = False
        
        # Stop optimization loop
        if self.optimization_task:
            self.optimization_task.cancel()
            try:
                await self.optimization_task
            except asyncio.CancelledError:
                pass
        
        # Stop core services
        await self.conductor_manager.stop()
        await self.queue_manager.stop()
        
        logger.info("âœ… LLM-Integrated Conductor Service stopped")
    
    async def deploy_clock_app(self) -> str:
        """Deploy the AI-generated clock application using the full graph architecture."""
        logger.info("ğŸ“± Deploying AI-generated clock application...")
        
        try:
            # Generate the clock app using LLM
            clock_app_code = await self._generate_clock_app_with_llm()
            
            if not clock_app_code:
                raise RuntimeError("Failed to generate clock app with LLM")
            
            # Create application ID
            app_id = f"clock_app_{uuid.uuid4().hex[:8]}"
            
            # Deploy the application components
            deployed_components = await self._deploy_app_components(app_id, clock_app_code)
            
            # Register with conductor
            await self._register_app_with_conductor(app_id, deployed_components)
            
            # Store deployment info
            self.deployed_apps[app_id] = {\n                'name': 'LLMFlow Clock App',\n                'deployed_at': datetime.utcnow(),\n                'components': deployed_components,\n                'generated_code': clock_app_code,\n                'performance_baseline': None,\n                'optimization_count': 0,\n                'last_optimization': None\n            }\n            \n            self.stats['apps_deployed'] += 1\n            \n            logger.info(f\"âœ… Clock app deployed successfully: {app_id}\")\n            return app_id\n            \n        except Exception as e:\n            logger.error(f\"âŒ Failed to deploy clock app: {e}\")\n            raise\n    \n    async def _generate_clock_app_with_llm(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Generate complete clock application using Gemini 2.0 Flash.\"\"\"\n        logger.info(\"ğŸ¤– Asking Gemini 2.0 Flash to generate clock application...\")\n        \n        try:\n            # Create detailed prompt for clock app generation\n            prompt = \"\"\"\nGenerate a complete real-time clock application using LLMFlow's graph-based architecture.\n\nRequirements:\n1. Use atoms for basic data types (TimeAtom, ClockStateAtom)\n2. Use molecules for business logic (ClockLogicMolecule, DisplayMolecule)\n3. Use conductors for runtime management\n4. Implement queue-based communication throughout\n5. Include real-time updates every second\n6. Add timezone support\n7. Include proper error handling and logging\n8. Make it production-ready with tests\n\nProvide response in JSON format:\n{\n  \"atoms\": {\n    \"TimeAtom\": \"complete Python code\",\n    \"ClockStateAtom\": \"complete Python code\"\n  },\n  \"molecules\": {\n    \"ClockLogicMolecule\": \"complete Python code\",\n    \"DisplayMolecule\": \"complete Python code\"\n  },\n  \"conductors\": {\n    \"ClockConductor\": \"complete Python code\"\n  },\n  \"main_app\": \"complete main.py code that ties everything together\",\n  \"deployment_config\": {\n    \"queues_needed\": [\"list of queue names\"],\n    \"dependencies\": [\"list of dependencies\"],\n    \"startup_order\": [\"component startup order\"]\n  }\n}\n\nMake this a fully functional, production-ready clock application.\n\"\"\"\n            \n            # Create OpenRouter request\n            request = OpenRouterRequest(\n                prompt=prompt,\n                model=self.llm_config['model'],\n                max_tokens=8000,\n                temperature=0.1,\n                site_url=\"https://llmflow.dev\",\n                site_name=\"LLMFlow Conductor\"\n            )\n            \n            # Send to LLM\n            response_atoms = await self.llm_service.process([OpenRouterRequestAtom(request)])\n            \n            if not response_atoms or response_atoms[0].response.error:\n                error = response_atoms[0].response.error if response_atoms else \"No response\"\n                logger.error(f\"LLM request failed: {error}\")\n                return None\n            \n            response = response_atoms[0].response\n            self.stats['total_llm_cost'] += response.cost_usd\n            \n            logger.info(f\"âœ… LLM generated clock app (${response.cost_usd:.4f}, confidence: {response.confidence_score:.2f})\")\n            \n            # Parse response\n            try:\n                clock_app_data = json.loads(response.content)\n                return clock_app_data\n            except json.JSONDecodeError:\n                logger.warning(\"LLM response not in JSON format, using raw response\")\n                return {\n                    'raw_response': response.content,\n                    'confidence': response.confidence_score\n                }\n        \n        except Exception as e:\n            logger.error(f\"Error generating clock app with LLM: {e}\")\n            return None\n    \n    async def _deploy_app_components(self, app_id: str, clock_app_code: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Deploy the application components to the runtime.\"\"\"\n        logger.info(f\"ğŸ—ï¸ Deploying components for {app_id}...\")\n        \n        deployed_components = {}\n        \n        try:\n            # Create app directory\n            app_dir = Path(f\"deployed_apps/{app_id}\")\n            app_dir.mkdir(parents=True, exist_ok=True)\n            \n            # Deploy atoms\n            if 'atoms' in clock_app_code:\n                for atom_name, atom_code in clock_app_code['atoms'].items():\n                    component_id = await self._deploy_component(\n                        app_id, atom_name, atom_code, ComponentType.ATOM, app_dir\n                    )\n                    deployed_components[atom_name] = component_id\n            \n            # Deploy molecules\n            if 'molecules' in clock_app_code:\n                for molecule_name, molecule_code in clock_app_code['molecules'].items():\n                    component_id = await self._deploy_component(\n                        app_id, molecule_name, molecule_code, ComponentType.MOLECULE, app_dir\n                    )\n                    deployed_components[molecule_name] = component_id\n            \n            # Deploy conductors\n            if 'conductors' in clock_app_code:\n                for conductor_name, conductor_code in clock_app_code['conductors'].items():\n                    component_id = await self._deploy_component(\n                        app_id, conductor_name, conductor_code, ComponentType.CELL, app_dir\n                    )\n                    deployed_components[conductor_name] = component_id\n            \n            # Create main app file\n            if 'main_app' in clock_app_code:\n                main_file = app_dir / \"main.py\"\n                with open(main_file, 'w') as f:\n                    f.write(clock_app_code['main_app'])\n                logger.info(f\"Created main app file: {main_file}\")\n            \n            # Setup queues if specified\n            if 'deployment_config' in clock_app_code:\n                deployment_config = clock_app_code['deployment_config']\n                if 'queues_needed' in deployment_config:\n                    await self._setup_app_queues(app_id, deployment_config['queues_needed'])\n            \n            logger.info(f\"âœ… Deployed {len(deployed_components)} components for {app_id}\")\n            return deployed_components\n        \n        except Exception as e:\n            logger.error(f\"Error deploying components for {app_id}: {e}\")\n            raise\n    \n    async def _deploy_component(self, app_id: str, component_name: str, \n                               component_code: str, component_type: ComponentType,\n                               app_dir: Path) -> str:\n        \"\"\"Deploy a single component.\"\"\"\n        try:\n            # Create component file\n            component_file = app_dir / f\"{component_name.lower()}.py\"\n            with open(component_file, 'w') as f:\n                f.write(f'\"\"\"\\n{component_name}\\nGenerated by LLM-Integrated Conductor\\n\"\"\"\\n\\n')\n                f.write(component_code)\n            \n            # Create a mock component for registration\n            from llmflow.core.base import Component\n            \n            class MockComponent(Component):\n                def __init__(self):\n                    super().__init__(component_name, component_type)\n                    self.code_file = str(component_file)\n            \n            component = MockComponent()\n            \n            # Register with conductor manager\n            process_id = await self.conductor_manager.register_process(component)\n            \n            # Start the process\n            await self.conductor_manager.start_process(process_id)\n            \n            logger.info(f\"âœ… Deployed component {component_name} (process: {process_id})\")\n            return process_id\n        \n        except Exception as e:\n            logger.error(f\"Error deploying component {component_name}: {e}\")\n            raise\n    \n    async def _setup_app_queues(self, app_id: str, queue_names: List[str]) -> None:\n        \"\"\"Setup queues needed by the application.\"\"\"\n        try:\n            for queue_name in queue_names:\n                # Create queue with app-specific domain\n                await self.queue_manager.create_queue(\n                    queue_name,\n                    domain=f\"app.{app_id}\"\n                )\n                logger.info(f\"Created queue: {queue_name} for app {app_id}\")\n        \n        except Exception as e:\n            logger.error(f\"Error setting up queues for {app_id}: {e}\")\n    \n    async def _register_app_with_conductor(self, app_id: str, components: Dict[str, str]) -> None:\n        \"\"\"Register the application with the conductor for monitoring.\"\"\"\n        try:\n            # Send app registration to system queue\n            await self.queue_manager.enqueue(\n                'system.app_registry',\n                {\n                    'app_id': app_id,\n                    'components': components,\n                    'deployed_at': datetime.utcnow().isoformat(),\n                    'conductor_id': self.conductor_manager.conductor_id\n                },\n                domain='system'\n            )\n            \n            logger.info(f\"âœ… Registered app {app_id} with conductor\")\n        \n        except Exception as e:\n            logger.error(f\"Error registering app {app_id}: {e}\")\n    \n    async def _llm_optimization_loop(self) -> None:\n        \"\"\"Main LLM optimization loop that monitors and improves applications.\"\"\"\n        logger.info(\"ğŸ§  Starting LLM optimization loop\")\n        \n        while self.running:\n            try:\n                await asyncio.sleep(self.llm_config['optimization_interval'])\n                \n                # Check each deployed application for optimization opportunities\n                for app_id, app_info in self.deployed_apps.items():\n                    await self._check_app_for_optimization(app_id, app_info)\n            \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                logger.error(f\"Error in LLM optimization loop: {e}\")\n        \n        logger.info(\"ğŸ§  LLM optimization loop stopped\")\n    \n    async def _check_app_for_optimization(self, app_id: str, app_info: Dict[str, Any]) -> None:\n        \"\"\"Check if an application needs optimization.\"\"\"\n        try:\n            # Get current performance metrics\n            current_performance = await self._get_app_performance(app_id)\n            \n            if not current_performance:\n                return\n            \n            # Check if we have a performance baseline\n            if not app_info['performance_baseline']:\n                app_info['performance_baseline'] = current_performance\n                logger.info(f\"ğŸ“Š Set performance baseline for {app_id}\")\n                return\n            \n            # Compare current performance to baseline\n            performance_degradation = self._calculate_performance_degradation(\n                app_info['performance_baseline'], \n                current_performance\n            )\n            \n            if performance_degradation > self.llm_config['performance_degradation_threshold']:\n                logger.info(f\"âš ï¸ Performance degradation detected for {app_id}: {performance_degradation:.1%}\")\n                await self._optimize_app_with_llm(app_id, app_info, current_performance)\n        \n        except Exception as e:\n            logger.error(f\"Error checking optimization for {app_id}: {e}\")\n    \n    async def _get_app_performance(self, app_id: str) -> Optional[Dict[str, float]]:\n        \"\"\"Get current performance metrics for an application.\"\"\"\n        try:\n            app_info = self.deployed_apps.get(app_id)\n            if not app_info:\n                return None\n            \n            # Aggregate performance from all components\n            total_latency = 0.0\n            total_memory = 0.0\n            total_cpu = 0.0\n            component_count = 0\n            \n            for component_name, process_id in app_info['components'].items():\n                metrics = await self.conductor_manager.get_process_metrics(process_id)\n                if metrics:\n                    total_latency += metrics.latency_ms\n                    total_memory += metrics.memory_usage_mb\n                    total_cpu += metrics.cpu_usage_percent\n                    component_count += 1\n            \n            if component_count == 0:\n                return None\n            \n            return {\n                'avg_latency_ms': total_latency / component_count,\n                'total_memory_mb': total_memory,\n                'avg_cpu_percent': total_cpu / component_count,\n                'component_count': component_count,\n                'timestamp': datetime.utcnow().isoformat()\n            }\n        \n        except Exception as e:\n            logger.error(f\"Error getting performance for {app_id}: {e}\")\n            return None\n    \n    def _calculate_performance_degradation(self, baseline: Dict[str, float], \n                                         current: Dict[str, float]) -> float:\n        \"\"\"Calculate performance degradation compared to baseline.\"\"\"\n        try:\n            # Calculate degradation based on key metrics\n            latency_degradation = (current['avg_latency_ms'] - baseline['avg_latency_ms']) / baseline['avg_latency_ms']\n            memory_degradation = (current['total_memory_mb'] - baseline['total_memory_mb']) / baseline['total_memory_mb']\n            cpu_degradation = (current['avg_cpu_percent'] - baseline['avg_cpu_percent']) / baseline['avg_cpu_percent']\n            \n            # Weighted average (latency is most important)\n            degradation = (latency_degradation * 0.5 + memory_degradation * 0.3 + cpu_degradation * 0.2)\n            \n            return max(0.0, degradation)  # Only positive degradation\n        \n        except (ZeroDivisionError, KeyError):\n            return 0.0\n    \n    async def _optimize_app_with_llm(self, app_id: str, app_info: Dict[str, Any], \n                                   current_performance: Dict[str, float]) -> None:\n        \"\"\"Use LLM to optimize the application.\"\"\"\n        logger.info(f\"ğŸ¤– Optimizing {app_id} with Gemini 2.0 Flash...\")\n        \n        try:\n            # Prepare optimization request\n            optimization_prompt = f\"\"\"\nOptimize this LLMFlow clock application based on performance analysis.\n\nCurrent Performance Issues:\n- Average Latency: {current_performance['avg_latency_ms']:.1f}ms\n- Total Memory Usage: {current_performance['total_memory_mb']:.1f}MB  \n- Average CPU Usage: {current_performance['avg_cpu_percent']:.1f}%\n- Component Count: {current_performance['component_count']}\n\nBaseline Performance:\n- Average Latency: {app_info['performance_baseline']['avg_latency_ms']:.1f}ms\n- Total Memory Usage: {app_info['performance_baseline']['total_memory_mb']:.1f}MB\n- Average CPU Usage: {app_info['performance_baseline']['avg_cpu_percent']:.1f}%\n\nExisting Application Code:\n{json.dumps(app_info['generated_code'], indent=2)[:2000]}...\n\nProvide optimized code that:\n1. Reduces latency and memory usage\n2. Improves CPU efficiency\n3. Maintains all functionality\n4. Is production-ready\n5. Includes specific performance improvements\n\nResponse format:\n{{\n  \"optimization_analysis\": \"detailed analysis of performance issues\",\n  \"optimized_components\": {{\n    \"component_name\": \"optimized code\"\n  }},\n  \"expected_improvements\": {{\n    \"latency_reduction\": \"percentage\",\n    \"memory_reduction\": \"percentage\",\n    \"cpu_reduction\": \"percentage\"\n  }},\n  \"confidence_score\": 0.0-1.0,\n  \"deployment_strategy\": \"how to deploy these optimizations safely\"\n}}\n\"\"\"\n            \n            # Send optimization request to LLM\n            request = OpenRouterRequest(\n                prompt=optimization_prompt,\n                model=self.llm_config['model'],\n                max_tokens=6000,\n                temperature=0.1,\n                site_url=\"https://llmflow.dev\",\n                site_name=\"LLMFlow Conductor Optimizer\"\n            )\n            \n            response_atoms = await self.llm_service.process([OpenRouterRequestAtom(request)])\n            \n            if not response_atoms or response_atoms[0].response.error:\n                error = response_atoms[0].response.error if response_atoms else \"No response\"\n                logger.error(f\"LLM optimization request failed: {error}\")\n                return\n            \n            response = response_atoms[0].response\n            self.stats['total_llm_cost'] += response.cost_usd\n            self.stats['optimizations_performed'] += 1\n            \n            logger.info(f\"âœ… LLM optimization analysis complete (${response.cost_usd:.4f})\")\n            \n            # Parse optimization response\n            try:\n                optimization_data = json.loads(response.content)\n                \n                # Check confidence score\n                confidence = optimization_data.get('confidence_score', 0.0)\n                \n                if confidence >= self.llm_config['auto_apply_threshold']:\n                    logger.info(f\"ğŸš€ Auto-applying optimization (confidence: {confidence:.1%})\")\n                    await self._apply_optimization(app_id, optimization_data)\n                    self.stats['auto_optimizations_applied'] += 1\n                else:\n                    logger.info(f\"âš ï¸ Optimization confidence too low: {confidence:.1%} (threshold: {self.llm_config['auto_apply_threshold']:.1%})\")\n                    # Store optimization for manual review\n                    await self._store_optimization_for_review(app_id, optimization_data)\n            \n            except json.JSONDecodeError:\n                logger.warning(\"LLM optimization response not in JSON format\")\n                # Store raw response for manual review\n                await self._store_optimization_for_review(app_id, {'raw_response': response.content})\n        \n        except Exception as e:\n            logger.error(f\"Error optimizing {app_id} with LLM: {e}\")\n    \n    async def _apply_optimization(self, app_id: str, optimization_data: Dict[str, Any]) -> None:\n        \"\"\"Apply LLM-generated optimization to the application.\"\"\"\n        logger.info(f\"ğŸ”§ Applying optimization to {app_id}...\")\n        \n        try:\n            app_info = self.deployed_apps[app_id]\n            optimized_components = optimization_data.get('optimized_components', {})\n            \n            # Apply optimizations component by component\n            for component_name, optimized_code in optimized_components.items():\n                if component_name in app_info['components']:\n                    process_id = app_info['components'][component_name]\n                    \n                    # Stop the current component\n                    await self.conductor_manager.stop_process(process_id)\n                    \n                    # Update the component code\n                    app_dir = Path(f\"deployed_apps/{app_id}\")\n                    component_file = app_dir / f\"{component_name.lower()}.py\"\n                    \n                    # Backup current version\n                    backup_file = app_dir / f\"{component_name.lower()}.py.backup_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n                    if component_file.exists():\n                        component_file.rename(backup_file)\n                    \n                    # Write optimized code\n                    with open(component_file, 'w') as f:\n                        f.write(f'\"\"\"\\n{component_name} - Optimized by LLM\\nOptimization applied: {datetime.utcnow()}\\n\"\"\"\\n\\n')\n                        f.write(optimized_code)\n                    \n                    # Restart the component\n                    await self.conductor_manager.start_process(process_id)\n                    \n                    logger.info(f\"âœ… Applied optimization to {component_name}\")\n            \n            # Update app info\n            app_info['optimization_count'] += 1\n            app_info['last_optimization'] = datetime.utcnow()\n            \n            # Reset performance baseline after optimization\n            app_info['performance_baseline'] = None\n            \n            logger.info(f\"âœ… Optimization applied successfully to {app_id}\")\n            \n        except Exception as e:\n            logger.error(f\"Error applying optimization to {app_id}: {e}\")\n            # Rollback on error\n            await self._rollback_optimization(app_id)\n    \n    async def _rollback_optimization(self, app_id: str) -> None:\n        \"\"\"Rollback a failed optimization.\"\"\"\n        logger.warning(f\"ğŸ”„ Rolling back optimization for {app_id}\")\n        \n        try:\n            app_dir = Path(f\"deployed_apps/{app_id}\")\n            \n            # Find and restore backup files\n            for backup_file in app_dir.glob(\"*.backup_*\"):\n                original_name = backup_file.name.split('.backup_')[0]\n                original_file = app_dir / original_name\n                \n                # Restore backup\n                backup_file.rename(original_file)\n                logger.info(f\"Restored {original_name} from backup\")\n            \n            # Restart all components\n            app_info = self.deployed_apps[app_id]\n            for component_name, process_id in app_info['components'].items():\n                await self.conductor_manager.restart_process(process_id)\n            \n            logger.info(f\"âœ… Rollback completed for {app_id}\")\n            \n        except Exception as e:\n            logger.error(f\"Error rolling back {app_id}: {e}\")\n    \n    async def _store_optimization_for_review(self, app_id: str, optimization_data: Dict[str, Any]) -> None:\n        \"\"\"Store optimization for manual review.\"\"\"\n        try:\n            review_dir = Path(\"optimization_reviews\")\n            review_dir.mkdir(exist_ok=True)\n            \n            review_file = review_dir / f\"{app_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json\"\n            \n            with open(review_file, 'w') as f:\n                json.dump({\n                    'app_id': app_id,\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'optimization_data': optimization_data,\n                    'requires_manual_review': True\n                }, f, indent=2)\n            \n            logger.info(f\"ğŸ“ Stored optimization for manual review: {review_file}\")\n            \n        except Exception as e:\n            logger.error(f\"Error storing optimization for review: {e}\")\n    \n    async def _on_process_started(self, event_type: str, event_data: Dict[str, Any]) -> None:\n        \"\"\"Handle process started events.\"\"\"\n        process_id = event_data.get('process_id')\n        component_name = event_data.get('component_name')\n        logger.info(f\"ğŸŸ¢ Process started: {component_name} ({process_id})\")\n    \n    async def _on_process_stopped(self, event_type: str, event_data: Dict[str, Any]) -> None:\n        \"\"\"Handle process stopped events.\"\"\"\n        process_id = event_data.get('process_id')\n        component_name = event_data.get('component_name')\n        logger.info(f\"ğŸ”´ Process stopped: {component_name} ({process_id})\")\n    \n    def get_service_status(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive service status.\"\"\"\n        uptime = None\n        if self.stats['uptime_start']:\n            uptime = (datetime.utcnow() - self.stats['uptime_start']).total_seconds()\n        \n        return {\n            'service': 'LLM-Integrated Conductor',\n            'running': self.running,\n            'uptime_seconds': uptime,\n            'api_key_configured': bool(self.api_key),\n            'llm_model': self.llm_config['model'],\n            'deployed_apps': list(self.deployed_apps.keys()),\n            'stats': self.stats.copy(),\n            'conductor_status': self.conductor_manager.get_conductor_status(),\n            'queue_manager_running': self.queue_manager.running if hasattr(self.queue_manager, 'running') else 'unknown'\n        }\n\n\n# Demo function to run the complete system\nasync def run_llm_conductor_demo():\n    \"\"\"Run a complete demo of the LLM-integrated conductor.\"\"\"\n    print(\"ğŸš€ Starting LLM-Integrated Conductor Demo\")\n    print(\"=\" * 50)\n    \n    # Initialize conductor\n    conductor = LLMIntegratedConductor()\n    \n    try:\n        # Start the service\n        print(\"ğŸ”§ Starting LLM-Integrated Conductor...\")\n        await conductor.start()\n        \n        # Deploy the clock app\n        print(\"\\nğŸ“± Deploying AI-generated clock application...\")\n        app_id = await conductor.deploy_clock_app()\n        print(f\"âœ… Clock app deployed: {app_id}\")\n        \n        # Show service status\n        print(\"\\nğŸ“Š Service Status:\")\n        status = conductor.get_service_status()\n        print(json.dumps(status, indent=2, default=str))\n        \n        # Let it run for a few minutes to show optimization\n        print(\"\\nâ³ Running for 2 minutes to demonstrate optimization...\")\n        print(\"   - Performance monitoring active\")\n        print(\"   - LLM optimization loop running\")\n        print(\"   - Watch for automatic optimizations!\")\n        \n        await asyncio.sleep(120)  # Run for 2 minutes\n        \n        # Final status\n        print(\"\\nğŸ“Š Final Status:\")\n        final_status = conductor.get_service_status()\n        print(json.dumps(final_status, indent=2, default=str))\n        \n    except KeyboardInterrupt:\n        print(\"\\nğŸ›‘ Demo interrupted by user\")\n    except Exception as e:\n        print(f\"\\nâŒ Demo error: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        # Clean shutdown\n        print(\"\\nğŸ›‘ Stopping LLM-Integrated Conductor...\")\n        await conductor.stop()\n        print(\"âœ… Demo completed!\")\n\n\nif __name__ == \"__main__\":\n    # Run the demo\n    try:\n        asyncio.run(run_llm_conductor_demo())\n    except KeyboardInterrupt:\n        print(\"\\nğŸ‘‹ Demo finished!\")\n