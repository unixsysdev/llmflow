"""
LLM Optimization Conductor

This module provides a conductor that integrates LLM optimization directly into
the LLMFlow framework. It monitors performance and automatically optimizes
components using OpenRouter + Gemini 2.0 Flash.
"""

import os
import json
import asyncio
import logging
import uuid
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime

from ..conductor.manager import ConductorManager
from ..queue.manager import QueueManager
from ..atoms.openrouter_llm import OpenRouterServiceAtom, OpenRouterRequest, OpenRouterRequestAtom
from ..atoms.data import StringAtom
from ..core.base import ComponentType, Component

logger = logging.getLogger(__name__)


class LLMOptimizationConductor(ConductorManager):
    """Enhanced conductor with integrated LLM optimization capabilities."""
    
    def __init__(self, queue_manager: QueueManager, security_enabled: bool = True):
        super().__init__(queue_manager, security_enabled)
        
        # LLM Integration
        self.openrouter_key = (
            os.getenv('OPENROUTER_API_KEY') or 
            "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"
        )
        os.environ['OPENROUTER_API_KEY'] = self.openrouter_key
        
        self.llm_service = OpenRouterServiceAtom()
        
        # LLM Configuration
        self.llm_config = {
            'model': 'google/gemini-2.0-flash-001',  # Only Gemini Flash
            'optimization_enabled': True,
            'auto_optimization_threshold': 0.85,
            'performance_check_interval': 30.0,  # Check performance every 30s
            'optimization_cooldown': 300.0,  # Wait 5min between optimizations
            'max_optimizations_per_component': 3
        }
        
        # LLM State
        self.llm_optimization_task: Optional[asyncio.Task] = None\n        self.component_optimization_history: Dict[str, List[Dict[str, Any]]] = {}\n        self.pending_optimizations: Dict[str, Dict[str, Any]] = {}\n        \n        # Enhanced metrics\n        self.llm_metrics = {\n            'optimizations_triggered': 0,\n            'optimizations_applied': 0,\n            'optimizations_rejected': 0,\n            'total_llm_cost': 0.0,\n            'performance_improvements': 0,\n            'components_optimized': set()\n        }\n        \n        logger.info(\"LLM Optimization Conductor initialized with Gemini 2.0 Flash\")\n    \n    async def start(self) -> None:\n        \"\"\"Start the LLM optimization conductor.\"\"\"\n        await super().start()\n        \n        if self.llm_config['optimization_enabled']:\n            # Start LLM optimization loop\n            self.llm_optimization_task = asyncio.create_task(self._llm_optimization_loop())\n            \n            # Subscribe to optimization requests from performance analysis\n            await self.queue_manager.subscribe(\n                'system.optimization_requests',\n                self._handle_optimization_request,\n                domain='system'\n            )\n            \n            logger.info(\"🤖 LLM optimization system started\")\n    \n    async def stop(self) -> None:\n        \"\"\"Stop the LLM optimization conductor.\"\"\"\n        if self.llm_optimization_task:\n            self.llm_optimization_task.cancel()\n            try:\n                await self.llm_optimization_task\n            except asyncio.CancelledError:\n                pass\n        \n        await super().stop()\n        logger.info(\"🤖 LLM optimization system stopped\")\n    \n    async def _llm_optimization_loop(self) -> None:\n        \"\"\"Main LLM optimization monitoring loop.\"\"\"\n        logger.info(\"🧠 LLM optimization loop started\")\n        \n        while self.running:\n            try:\n                await asyncio.sleep(self.llm_config['performance_check_interval'])\n                \n                # Check all managed processes for optimization opportunities\n                for process_id, process_info in self.managed_processes.items():\n                    if process_info.status.value == 'running':\n                        await self._check_process_for_llm_optimization(process_id)\n            \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                logger.error(f\"Error in LLM optimization loop: {e}\")\n        \n        logger.info(\"🧠 LLM optimization loop stopped\")\n    \n    async def _check_process_for_llm_optimization(self, process_id: str) -> None:\n        \"\"\"Check if a process needs LLM optimization.\"\"\"\n        try:\n            process_info = self.managed_processes.get(process_id)\n            if not process_info:\n                return\n            \n            # Skip if already being optimized\n            if process_id in self.pending_optimizations:\n                return\n            \n            # Check optimization history\n            optimization_count = len(self.component_optimization_history.get(process_id, []))\n            if optimization_count >= self.llm_config['max_optimizations_per_component']:\n                return\n            \n            # Check cooldown period\n            if self._is_in_optimization_cooldown(process_id):\n                return\n            \n            # Get current performance metrics\n            current_metrics = await self.get_process_metrics(process_id)\n            if not current_metrics:\n                return\n            \n            # Check if optimization is needed based on performance\n            needs_optimization = await self._evaluate_optimization_need(process_id, current_metrics)\n            \n            if needs_optimization:\n                logger.info(f\"🎯 Triggering LLM optimization for process {process_id}\")\n                await self._trigger_llm_optimization(process_id, current_metrics)\n        \n        except Exception as e:\n            logger.error(f\"Error checking LLM optimization for {process_id}: {e}\")\n    \n    def _is_in_optimization_cooldown(self, process_id: str) -> bool:\n        \"\"\"Check if process is in optimization cooldown period.\"\"\"\n        history = self.component_optimization_history.get(process_id, [])\n        if not history:\n            return False\n        \n        last_optimization = history[-1]\n        last_time = datetime.fromisoformat(last_optimization['timestamp'])\n        cooldown_seconds = self.llm_config['optimization_cooldown']\n        \n        return (datetime.utcnow() - last_time).total_seconds() < cooldown_seconds\n    \n    async def _evaluate_optimization_need(self, process_id: str, current_metrics) -> bool:\n        \"\"\"Evaluate if a process needs optimization based on performance.\"\"\"\n        try:\n            # Get performance history for comparison\n            history = self.performance_history.get(process_id, [])\n            \n            if len(history) < 5:  # Need some history for comparison\n                return False\n            \n            # Calculate performance degradation\n            recent_avg_latency = sum(h.get('latency_ms', 0) for h in history[-5:]) / 5\n            baseline_avg_latency = sum(h.get('latency_ms', 0) for h in history[:5]) / 5\n            \n            # Check for significant performance degradation\n            if baseline_avg_latency > 0:\n                degradation = (recent_avg_latency - baseline_avg_latency) / baseline_avg_latency\n                \n                # Trigger optimization if performance degraded by >20%\n                if degradation > 0.2:\n                    logger.info(f\"Performance degradation detected: {degradation:.1%}\")\n                    return True\n            \n            # Check for high error rates\n            recent_error_rate = current_metrics.error_rate\n            if recent_error_rate > 0.05:  # >5% error rate\n                logger.info(f\"High error rate detected: {recent_error_rate:.1%}\")\n                return True\n            \n            # Check for memory leaks\n            memory_values = [h.get('memory_usage_mb', 0) for h in history[-10:]]\n            if len(memory_values) >= 5:\n                memory_growth = self._calculate_growth_rate(memory_values)\n                if memory_growth > 0.1:  # >10% memory growth\n                    logger.info(f\"Memory growth detected: {memory_growth:.1%}\")\n                    return True\n            \n            return False\n        \n        except Exception as e:\n            logger.error(f\"Error evaluating optimization need: {e}\")\n            return False\n    \n    async def _trigger_llm_optimization(self, process_id: str, current_metrics) -> None:\n        \"\"\"Trigger LLM optimization for a process.\"\"\"\n        try:\n            process_info = self.managed_processes.get(process_id)\n            if not process_info:\n                return\n            \n            # Mark as pending optimization\n            self.pending_optimizations[process_id] = {\n                'started_at': datetime.utcnow().isoformat(),\n                'metrics': {\n                    'latency_ms': current_metrics.latency_ms,\n                    'memory_usage_mb': current_metrics.memory_usage_mb,\n                    'cpu_usage_percent': current_metrics.cpu_usage_percent,\n                    'error_rate': current_metrics.error_rate\n                }\n            }\n            \n            # Get component code for analysis\n            component_code = await self._get_component_code(process_id)\n            \n            # Create optimization prompt\n            optimization_prompt = self._create_optimization_prompt(\n                process_id, \n                process_info.component_name, \n                current_metrics, \n                component_code\n            )\n            \n            # Send to LLM\n            await self._send_optimization_request_to_llm(process_id, optimization_prompt)\n            \n            self.llm_metrics['optimizations_triggered'] += 1\n            \n        except Exception as e:\n            logger.error(f\"Error triggering LLM optimization for {process_id}: {e}\")\n            # Remove from pending on error\n            self.pending_optimizations.pop(process_id, None)\n    \n    async def _get_component_code(self, process_id: str) -> str:\n        \"\"\"Get the source code for a component (mock implementation).\"\"\"\n        # In a real implementation, this would fetch the actual component source code\n        # For demo purposes, return a mock component\n        return f\"\"\"\n# Component code for process {process_id}\nclass MockComponent:\n    def __init__(self):\n        self.data = []\n    \n    def process(self, input_data):\n        # Inefficient processing - could be optimized\n        result = []\n        for item in input_data:\n            # Slow operation that could be vectorized\n            for i in range(len(self.data)):\n                if self.data[i] == item:\n                    result.append(item * 2)\n                    break\n        return result\n    \n    def add_data(self, item):\n        # Inefficient - creates new list each time\n        self.data = self.data + [item]\n\"\"\"\n    \n    def _create_optimization_prompt(self, process_id: str, component_name: str, \n                                  metrics, component_code: str) -> str:\n        \"\"\"Create optimization prompt for LLM.\"\"\"\n        performance_history = self.performance_history.get(process_id, [])\n        \n        prompt = f\"\"\"\nOptimize this LLMFlow component for better performance.\n\n**Component Details:**\n- Process ID: {process_id}\n- Component Name: {component_name}\n- Current Performance Issues:\n  * Latency: {metrics.latency_ms:.1f}ms\n  * Memory Usage: {metrics.memory_usage_mb:.1f}MB\n  * CPU Usage: {metrics.cpu_usage_percent:.1f}%\n  * Error Rate: {metrics.error_rate:.1%}\n\n**Performance History (last 5 measurements):**\n{json.dumps(performance_history[-5:], indent=2)}\n\n**Current Component Code:**\n```python\n{component_code}\n```\n\n**Optimization Requirements:**\n1. Reduce latency and memory usage\n2. Fix any performance bottlenecks\n3. Maintain API compatibility\n4. Add proper error handling\n5. Use efficient algorithms and data structures\n6. Keep code readable and maintainable\n\n**Response Format (JSON):**\n{{\n  \"analysis\": \"detailed analysis of performance issues\",\n  \"optimized_code\": \"complete optimized Python code\",\n  \"improvements\": {\n    \"latency_improvement\": \"expected percentage improvement\",\n    \"memory_improvement\": \"expected percentage improvement\",\n    \"cpu_improvement\": \"expected percentage improvement\"\n  },\n  \"confidence_score\": 0.0-1.0,\n  \"breaking_changes\": [\"list any breaking changes\"],\n  \"deployment_notes\": \"how to deploy this optimization safely\"\n}}\n\nGenerate production-ready optimized code that significantly improves performance.\n\"\"\"\n        \n        return prompt\n    \n    async def _send_optimization_request_to_llm(self, process_id: str, prompt: str) -> None:\n        \"\"\"Send optimization request to LLM.\"\"\"\n        try:\n            # Create OpenRouter request\n            request = OpenRouterRequest(\n                prompt=prompt,\n                model=self.llm_config['model'],\n                max_tokens=4000,\n                temperature=0.1,\n                site_url=\"https://llmflow.dev\",\n                site_name=\"LLMFlow Conductor Optimizer\"\n            )\n            \n            logger.info(f\"🤖 Sending optimization request to Gemini 2.0 Flash...\")\n            \n            # Process with LLM\n            response_atoms = await self.llm_service.process([OpenRouterRequestAtom(request)])\n            \n            if not response_atoms or response_atoms[0].response.error:\n                error = response_atoms[0].response.error if response_atoms else \"No response\"\n                logger.error(f\"LLM optimization request failed: {error}\")\n                self.llm_metrics['optimizations_rejected'] += 1\n                return\n            \n            response = response_atoms[0].response\n            self.llm_metrics['total_llm_cost'] += response.cost_usd\n            \n            logger.info(f\"✅ LLM optimization response received (${response.cost_usd:.4f}, confidence: {response.confidence_score:.2f})\")\n            \n            # Process the optimization response\n            await self._process_optimization_response(process_id, response.content)\n        \n        except Exception as e:\n            logger.error(f\"Error sending LLM optimization request: {e}\")\n            self.llm_metrics['optimizations_rejected'] += 1\n        finally:\n            # Remove from pending\n            self.pending_optimizations.pop(process_id, None)\n    \n    async def _process_optimization_response(self, process_id: str, response_content: str) -> None:\n        \"\"\"Process LLM optimization response.\"\"\"\n        try:\n            # Parse JSON response\n            optimization_data = json.loads(response_content)\n            \n            confidence = optimization_data.get('confidence_score', 0.0)\n            optimized_code = optimization_data.get('optimized_code', '')\n            analysis = optimization_data.get('analysis', '')\n            \n            logger.info(f\"📊 Optimization analysis: {analysis[:100]}...\")\n            \n            # Check if confidence meets threshold for auto-application\n            if confidence >= self.llm_config['auto_optimization_threshold']:\n                logger.info(f\"🚀 Auto-applying optimization (confidence: {confidence:.1%})\")\n                await self._apply_optimization(process_id, optimization_data)\n                self.llm_metrics['optimizations_applied'] += 1\n                self.llm_metrics['components_optimized'].add(process_id)\n            else:\n                logger.info(f\"⚠️ Optimization confidence too low: {confidence:.1%} (threshold: {self.llm_config['auto_optimization_threshold']:.1%})\")\n                await self._store_optimization_for_manual_review(process_id, optimization_data)\n                self.llm_metrics['optimizations_rejected'] += 1\n        \n        except json.JSONDecodeError:\n            logger.warning(\"LLM response not in JSON format\")\n            await self._store_optimization_for_manual_review(process_id, {'raw_response': response_content})\n            self.llm_metrics['optimizations_rejected'] += 1\n        except Exception as e:\n            logger.error(f\"Error processing optimization response: {e}\")\n            self.llm_metrics['optimizations_rejected'] += 1\n    \n    async def _apply_optimization(self, process_id: str, optimization_data: Dict[str, Any]) -> None:\n        \"\"\"Apply LLM optimization to a component.\"\"\"\n        try:\n            logger.info(f\"🔧 Applying LLM optimization to process {process_id}...\")\n            \n            process_info = self.managed_processes.get(process_id)\n            if not process_info:\n                logger.error(f\"Process {process_id} not found\")\n                return\n            \n            # In a real implementation, this would:\n            # 1. Create a backup of the current component\n            # 2. Deploy the optimized code\n            # 3. Restart the component\n            # 4. Monitor for successful deployment\n            \n            # For demo purposes, we'll simulate the optimization\n            logger.info(f\"   📦 Creating backup of component {process_info.component_name}\")\n            logger.info(f\"   🔄 Deploying optimized code\")\n            \n            # Simulate restart\n            await self.restart_process(process_id)\n            \n            # Record optimization in history\n            if process_id not in self.component_optimization_history:\n                self.component_optimization_history[process_id] = []\n            \n            self.component_optimization_history[process_id].append({\n                'timestamp': datetime.utcnow().isoformat(),\n                'optimization_data': optimization_data,\n                'confidence': optimization_data.get('confidence_score', 0.0),\n                'expected_improvements': optimization_data.get('improvements', {})\n            })\n            \n            logger.info(f\"✅ Optimization applied successfully to {process_id}\")\n            \n            # Reset performance baseline to measure improvement\n            if process_id in self.performance_history:\n                self.performance_history[process_id] = []\n        \n        except Exception as e:\n            logger.error(f\"Error applying optimization to {process_id}: {e}\")\n            # In a real implementation, this would trigger rollback\n    \n    async def _store_optimization_for_manual_review(self, process_id: str, optimization_data: Dict[str, Any]) -> None:\n        \"\"\"Store optimization for manual review.\"\"\"\n        try:\n            # Send to manual review queue\n            await self.queue_manager.enqueue(\n                'system.optimization_review',\n                {\n                    'process_id': process_id,\n                    'conductor_id': self.conductor_id,\n                    'optimization_data': optimization_data,\n                    'timestamp': datetime.utcnow().isoformat(),\n                    'requires_manual_review': True\n                },\n                domain='system'\n            )\n            \n            logger.info(f\"📝 Stored optimization for manual review: {process_id}\")\n        \n        except Exception as e:\n            logger.error(f\"Error storing optimization for review: {e}\")\n    \n    async def _handle_optimization_request(self, message: Dict[str, Any]) -> None:\n        \"\"\"Handle optimization requests from the performance analysis system.\"\"\"\n        try:\n            process_id = message.get('process_id')\n            anomalies = message.get('anomalies', [])\n            \n            if not process_id:\n                return\n            \n            logger.info(f\"📨 Received optimization request for {process_id}: {anomalies}\")\n            \n            # Get current metrics\n            current_metrics = await self.get_process_metrics(process_id)\n            if current_metrics:\n                await self._trigger_llm_optimization(process_id, current_metrics)\n        \n        except Exception as e:\n            logger.error(f\"Error handling optimization request: {e}\")\n    \n    def get_llm_optimization_status(self) -> Dict[str, Any]:\n        \"\"\"Get LLM optimization status and metrics.\"\"\"\n        return {\n            'llm_optimization_enabled': self.llm_config['optimization_enabled'],\n            'llm_model': self.llm_config['model'],\n            'api_key_configured': bool(self.openrouter_key),\n            'pending_optimizations': len(self.pending_optimizations),\n            'components_with_history': len(self.component_optimization_history),\n            'metrics': {\n                **self.llm_metrics,\n                'components_optimized': len(self.llm_metrics['components_optimized'])\n            },\n            'config': self.llm_config.copy()\n        }\n    \n    def get_conductor_status(self) -> Dict[str, Any]:\n        \"\"\"Get enhanced conductor status including LLM optimization.\"\"\"\n        base_status = super().get_conductor_status()\n        base_status['llm_optimization'] = self.get_llm_optimization_status()\n        return base_status\n